"use strict";(self.webpackChunkmongodb_rag_docs=self.webpackChunkmongodb_rag_docs||[]).push([[7624],{4168:function(n,e,t){t.r(e),t.d(e,{assets:function(){return c},contentTitle:function(){return i},default:function(){return d},frontMatter:function(){return o},metadata:function(){return r},toc:function(){return l}});var r=JSON.parse('{"id":"workshop/build-rag-app/advanced-techniques","title":"\ud83d\udc50 Advanced RAG Techniques","description":"Now that you\'ve built a basic RAG application, let\'s explore advanced techniques to enhance its performance, relevance, and efficiency.","source":"@site/docs/workshop/50-build-rag-app/3-perform-vector-search.mdx","sourceDirName":"workshop/50-build-rag-app","slug":"/workshop/build-rag-app/advanced-techniques","permalink":"/mongodb-rag/docs/workshop/build-rag-app/advanced-techniques","draft":false,"unlisted":false,"editUrl":"https://github.com/mongodb-developer/mongodb-rag/tree/main/mongodb-rag-docs/docs/workshop/50-build-rag-app/3-perform-vector-search.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"advanced-techniques","title":"\ud83d\udc50 Advanced RAG Techniques"}}'),s=t(4848),a=t(8453);const o={id:"advanced-techniques",title:"\ud83d\udc50 Advanced RAG Techniques"},i="Advanced RAG Techniques",c={},l=[{value:"Hybrid Search: Combining Vector and Keyword Search",id:"hybrid-search-combining-vector-and-keyword-search",level:2},{value:"Implementation",id:"implementation",level:3},{value:"Benefits of Hybrid Search",id:"benefits-of-hybrid-search",level:3},{value:"Advanced Document Chunking",id:"advanced-document-chunking",level:2},{value:"Metadata Filtering",id:"metadata-filtering",level:2},{value:"Re-ranking Search Results",id:"re-ranking-search-results",level:2},{value:"Vector Quantization",id:"vector-quantization",level:2},{value:"Multi-stage RAG Pipeline",id:"multi-stage-rag-pipeline",level:2},{value:"Self-Critique and Refinement",id:"self-critique-and-refinement",level:2},{value:"Next Steps",id:"next-steps",level:2}];function u(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"advanced-rag-techniques",children:"Advanced RAG Techniques"})}),"\n",(0,s.jsx)(e.p,{children:"Now that you've built a basic RAG application, let's explore advanced techniques to enhance its performance, relevance, and efficiency."}),"\n",(0,s.jsx)(e.h2,{id:"hybrid-search-combining-vector-and-keyword-search",children:"Hybrid Search: Combining Vector and Keyword Search"}),"\n",(0,s.jsx)(e.p,{children:"MongoDB Atlas supports hybrid search, which combines the strengths of vector search with traditional keyword search."}),"\n",(0,s.jsx)(e.h3,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(e.p,{children:"Update your search function to use hybrid search:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:'async function hybridSearch(query, options = {}) {\n  try {\n    await rag.connect();\n    \n    // Get the embedding for the query\n    const embedding = await rag.provider.getEmbedding(query);\n    \n    // Build the aggregation pipeline\n    const collection = await rag._getCollection();\n    const pipeline = [\n      {\n        $search: {\n          compound: {\n            should: [\n              {\n                vectorSearch: {\n                  queryVector: embedding,\n                  path: "embedding",\n                  numCandidates: 100,\n                  limit: 10\n                }\n              },\n              {\n                text: {\n                  query: query,\n                  path: "content"\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        $project: {\n          documentId: 1,\n          content: 1,\n          metadata: 1,\n          score: { $meta: "searchScore" }\n        }\n      },\n      {\n        $limit: options.maxResults || 5\n      }\n    ];\n    \n    // Execute the search\n    const results = await collection.aggregate(pipeline).toArray();\n    \n    return results.map(r => ({\n      documentId: r.documentId,\n      content: r.content,\n      metadata: r.metadata,\n      score: r.score\n    }));\n    \n  } catch (error) {\n    console.error(\'Hybrid search error:\', error);\n    throw error;\n  } finally {\n    await rag.close();\n  }\n}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"benefits-of-hybrid-search",children:"Benefits of Hybrid Search"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Improved recall"}),": Finds relevant documents that might be missed by vector search alone"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Better handling of specialized terms"}),": Exact keyword matching for technical terms, product names, etc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reduced sensitivity to embedding quality"}),": Less dependent on embedding model quality"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-language support"}),": Works well across languages and specialized domains"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"advanced-document-chunking",children:"Advanced Document Chunking"}),"\n",(0,s.jsx)(e.p,{children:"Let's implement a more sophisticated chunking strategy that respects document structure:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:"const { MongoRAG } = require('mongodb-rag');\nconst natural = require('natural');\n\nclass AdvancedChunker {\n  constructor(options = {}) {\n    this.options = {\n      maxChunkSize: options.maxChunkSize || 500,\n      minChunkSize: options.minChunkSize || 100,\n      overlap: options.overlap || 50,\n      ...options\n    };\n    \n    this.sentenceTokenizer = new natural.SentenceTokenizer();\n  }\n  \n  async chunkDocument(document) {\n    const chunks = [];\n    const content = document.content;\n    \n    // Step 1: Split content by headings\n    const sections = this._splitByHeadings(content);\n    \n    // Step 2: Process each section\n    for (const section of sections) {\n      // If section is already small enough, keep it as is\n      if (section.length <= this.options.maxChunkSize) {\n        chunks.push(this._createChunk(document, section));\n        continue;\n      }\n      \n      // Step 3: Split large sections into paragraphs\n      const paragraphs = section.split(/\\n\\s*\\n/);\n      let currentChunk = '';\n      \n      for (const paragraph of paragraphs) {\n        // If paragraph fits in current chunk, add it\n        if (currentChunk.length + paragraph.length <= this.options.maxChunkSize) {\n          currentChunk += (currentChunk ? '\\n\\n' : '') + paragraph;\n        } else {\n          // If current chunk is not empty, save it\n          if (currentChunk) {\n            chunks.push(this._createChunk(document, currentChunk));\n          }\n          \n          // If paragraph is too large, split into sentences\n          if (paragraph.length > this.options.maxChunkSize) {\n            const sentenceChunks = this._chunkBySentences(paragraph);\n            chunks.push(...sentenceChunks.map(c => this._createChunk(document, c)));\n          } else {\n            currentChunk = paragraph;\n          }\n        }\n      }\n      \n      // Save the last chunk if not empty\n      if (currentChunk) {\n        chunks.push(this._createChunk(document, currentChunk));\n      }\n    }\n    \n    return chunks;\n  }\n  \n  _splitByHeadings(content) {\n    // Split by markdown headings (##, ###, etc.)\n    return content.split(/\\n#{1,6}\\s+[^\\n]+/);\n  }\n  \n  _chunkBySentences(text) {\n    const sentences = this.sentenceTokenizer.tokenize(text);\n    const chunks = [];\n    let currentChunk = '';\n    \n    for (const sentence of sentences) {\n      if (currentChunk.length + sentence.length <= this.options.maxChunkSize) {\n        currentChunk += (currentChunk ? ' ' : '') + sentence;\n      } else {\n        if (currentChunk) {\n          chunks.push(currentChunk);\n        }\n        currentChunk = sentence;\n      }\n    }\n    \n    if (currentChunk) {\n      chunks.push(currentChunk);\n    }\n    \n    return chunks;\n  }\n  \n  _createChunk(document, content) {\n    return {\n      documentId: document.id,\n      content: content,\n      metadata: {\n        ...document.metadata,\n        chunkIndex: Date.now(),\n        chunkLength: content.length\n      }\n    };\n  }\n}\n\nmodule.exports = AdvancedChunker;\n"})}),"\n",(0,s.jsx)(e.h2,{id:"metadata-filtering",children:"Metadata Filtering"}),"\n",(0,s.jsx)(e.p,{children:"Metadata filtering allows you to narrow down search results based on document metadata:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:'async function searchWithMetadata(query, metadata = {}, options = {}) {\n  try {\n    await rag.connect();\n    \n    // Build metadata filter\n    const filter = {};\n    Object.entries(metadata).forEach(([key, value]) => {\n      filter[`metadata.${key}`] = value;\n    });\n    \n    // Perform search with filter\n    const results = await rag.search(query, {\n      filter: filter,\n      maxResults: options.maxResults || 5\n    });\n    \n    return results;\n    \n  } catch (error) {\n    console.error(\'Metadata search error:\', error);\n    throw error;\n  } finally {\n    await rag.close();\n  }\n}\n\n// Example usage\nconst results = await searchWithMetadata(\n  "What is MongoDB Atlas?",\n  { type: "markdown", source: "mongodb-atlas.md" }\n);\n'})}),"\n",(0,s.jsx)(e.h2,{id:"re-ranking-search-results",children:"Re-ranking Search Results"}),"\n",(0,s.jsx)(e.p,{children:"Improve search relevance by re-ranking results:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:"const { MongoRAG } = require('mongodb-rag');\nconst { OpenAI } = require('openai');\nconst config = require('./config');\n\n// Initialize OpenAI client\nconst openai = new OpenAI({\n  apiKey: config.llm.apiKey\n});\n\nasync function searchWithReranking(query, options = {}) {\n  try {\n    // Step 1: Get initial results (more than needed)\n    const initialResults = await rag.search(query, {\n      maxResults: (options.maxResults || 5) * 3\n    });\n    \n    if (initialResults.length === 0) {\n      return [];\n    }\n    \n    // Step 2: Re-rank results using the LLM\n    const rerankerPrompt = `\nQuery: \"${query}\"\n\nI'll show you ${initialResults.length} text passages. Rank them based on relevance to the query.\nFor each passage, assign a score from 0-10 where:\n- 10: Perfect match, directly answers the query\n- 7-9: Highly relevant, contains most of the answer\n- 4-6: Somewhat relevant, contains partial information\n- 1-3: Slightly relevant, mentions related concepts\n- 0: Not relevant at all\n\nFor each passage, output only the score followed by brief explanation.\n\n${initialResults.map((result, i) => \n  `PASSAGE ${i+1}: \"${result.content.substring(0, 300)}...\"`\n).join('\\n\\n')}\n`;\n    \n    const completion = await openai.chat.completions.create({\n      model: \"gpt-3.5-turbo\",\n      messages: [\n        { role: \"system\", content: \"You are a helpful assistant that ranks search results based on relevance.\" },\n        { role: \"user\", content: rerankerPrompt }\n      ],\n      temperature: 0.3\n    });\n    \n    // Step 3: Parse LLM response to get scores\n    const scores = parseRerankerResponse(completion.choices[0].message.content);\n    \n    // Step 4: Combine original results with new scores\n    const scoredResults = initialResults.map((result, i) => ({\n      ...result,\n      reranked_score: scores[i] || 0\n    }));\n    \n    // Step 5: Sort by new scores and return top results\n    const rerankedResults = scoredResults\n      .sort((a, b) => b.reranked_score - a.reranked_score)\n      .slice(0, options.maxResults || 5);\n    \n    return rerankedResults;\n    \n  } catch (error) {\n    console.error('Reranking error:', error);\n    throw error;\n  }\n}\n\n// Helper function to parse reranker response\nfunction parseRerankerResponse(response) {\n  const scores = [];\n  const regex = /PASSAGE\\s*(\\d+)[^0-9]*(\\d+)/gi;\n  let match;\n  \n  while ((match = regex.exec(response)) !== null) {\n    const passageNum = parseInt(match[1]) - 1;\n    const score = parseInt(match[2]);\n    scores[passageNum] = score;\n  }\n  \n  return scores;\n}\n\n## Query Expansion\n\nAnother technique to improve search results is query expansion. This involves generating multiple related queries to capture different ways of expressing the same information need:\n\n```javascript\nconst { OpenAI } = require('openai');\nconst config = require('./config');\n\n// Initialize OpenAI client\nconst openai = new OpenAI({\n  apiKey: config.llm.apiKey\n});\n\nasync function expandQuery(query) {\n  // Use LLM to generate query variations\n  const completion = await openai.chat.completions.create({\n    model: 'gpt-3.5-turbo',\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a search expert. Generate 3-5 variations of the query that express the same information need in different ways. Return ONLY the variations, one per line.'\n      },\n      {\n        role: 'user',\n        content: `Original query: \"${query}\"`\n      }\n    ],\n    temperature: 0.7\n  });\n  \n  // Parse the response to get query variations\n  const variations = completion.choices[0].message.content\n    .split('\\n')\n    .map(v => v.trim())\n    .filter(v => v && !v.startsWith('Original') && !v.startsWith('-'));\n  \n  return [query, ...variations];\n}\n\nasync function searchWithQueryExpansion(query, options = {}) {\n  try {\n    // Step 1: Generate query variations\n    const expandedQueries = await expandQuery(query);\n    console.log('Expanded queries:', expandedQueries);\n    \n    // Step 2: Search with each variation\n    const allResults = [];\n    for (const q of expandedQueries) {\n      const results = await rag.search(q, {\n        maxResults: options.maxResults || 3\n      });\n      allResults.push(...results);\n    }\n    \n    // Step 3: Deduplicate results based on documentId\n    const uniqueResults = [];\n    const seenIds = new Set();\n    \n    for (const result of allResults) {\n      if (!seenIds.has(result.documentId)) {\n        seenIds.add(result.documentId);\n        uniqueResults.push(result);\n      }\n    }\n    \n    // Step 4: Sort by score and return top results\n    return uniqueResults\n      .sort((a, b) => b.score - a.score)\n      .slice(0, options.maxResults || 5);\n    \n  } catch (error) {\n    console.error('Query expansion error:', error);\n    throw error;\n  }\n}\n"})}),"\n",(0,s.jsx)(e.h2,{id:"vector-quantization",children:"Vector Quantization"}),"\n",(0,s.jsx)(e.p,{children:"MongoDB Atlas supports vector quantization to improve search performance with minimal impact on quality:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:'const { MongoClient } = require(\'mongodb\');\nconst config = require(\'./config\');\n\nasync function createQuantizedIndex() {\n  const client = new MongoClient(config.mongodb.uri);\n  \n  try {\n    await client.connect();\n    const database = client.db(config.mongodb.database);\n    const collection = database.collection(config.mongodb.collection);\n    \n    // Define scalar quantization index\n    const scalarQuantizationIndex = {\n      name: "vector_quantized_index",\n      type: "vectorSearch",\n      definition: {\n        fields: [\n          {\n            type: "vector",\n            path: "embedding",\n            numDimensions: config.embedding.dimensions,\n            similarity: "cosine",\n            quantization: {\n              type: "scalar",\n              config: {\n                components: { type: "int8" }\n              }\n            }\n          }\n        ]\n      }\n    };\n    \n    // Create the quantized index\n    console.log(\'Creating quantized vector search index...\');\n    const result = await collection.createSearchIndex(scalarQuantizationIndex);\n    console.log(\'Quantized index creation initiated:\', result);\n    \n  } catch (error) {\n    console.error(\'Error creating quantized index:\', error);\n    throw error;\n  } finally {\n    await client.close();\n  }\n}\n'})}),"\n",(0,s.jsx)(e.p,{children:"Vector quantization offers these benefits:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reduced storage requirements"}),"\n",(0,s.jsx)(e.li,{children:"Improved search performance"}),"\n",(0,s.jsx)(e.li,{children:"Minimal impact on recall/precision"}),"\n",(0,s.jsx)(e.li,{children:"Better scaling for large collections"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"multi-stage-rag-pipeline",children:"Multi-stage RAG Pipeline"}),"\n",(0,s.jsx)(e.p,{children:"Implement a multi-stage RAG pipeline that combines multiple retrieval and filtering techniques:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:"async function advancedRAGPipeline(query, options = {}) {\n  try {\n    // Step 1: Extract entities and intent from query\n    const queryAnalysis = await analyzeQuery(query);\n    \n    // Step 2: Expand query with variations\n    const expandedQueries = await expandQuery(query);\n    \n    // Step 3: Initial retrieval from multiple sources\n    const initialResults = await retrieveFromMultipleSources(expandedQueries, queryAnalysis);\n    \n    // Step 4: Filter by metadata based on query intent\n    const filteredResults = filterByMetadata(initialResults, queryAnalysis.metadata);\n    \n    // Step 5: Rerank results\n    const rerankedResults = await rerankResults(filteredResults, query, queryAnalysis);\n    \n    // Step 6: Dynamic prompt construction based on retrieved context\n    const prompt = constructPrompt(query, rerankedResults, queryAnalysis);\n    \n    // Step 7: Generate response with the LLM\n    const response = await generateLLMResponse(prompt);\n    \n    return {\n      answer: response,\n      sources: rerankedResults.map(r => ({\n        documentId: r.documentId,\n        source: r.metadata?.source,\n        score: r.reranked_score || r.score\n      }))\n    };\n    \n  } catch (error) {\n    console.error('Advanced RAG pipeline error:', error);\n    throw error;\n  }\n}\n"})}),"\n",(0,s.jsx)(e.h2,{id:"self-critique-and-refinement",children:"Self-Critique and Refinement"}),"\n",(0,s.jsx)(e.p,{children:"Enhance response quality through self-critique and refinement:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-javascript",children:"async function generateWithSelfCritique(query, context) {\n  // Step 1: Generate initial response\n  const initialResponse = await openai.chat.completions.create({\n    model: config.llm.model,\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a helpful assistant. Answer based only on the provided context.'\n      },\n      {\n        role: 'user',\n        content: `Context:\\n${context}\\n\\nQuestion: ${query}`\n      }\n    ]\n  });\n  \n  // Step 2: Self-critique the response\n  const critique = await openai.chat.completions.create({\n    model: config.llm.model,\n    messages: [\n      {\n        role: 'system',\n        content: `You are a critical evaluator. Assess the following answer for:\n1. Factual accuracy based on the context\n2. Completeness of information\n3. Logical coherence\n4. Possible hallucinations or unsupported claims\nProvide specific suggestions for improvement.`\n      },\n      {\n        role: 'user',\n        content: `Context:\\n${context}\\n\\nQuestion: ${query}\\n\\nAnswer: ${initialResponse.choices[0].message.content}`\n      }\n    ]\n  });\n  \n  // Step 3: Refine the response based on critique\n  const refinedResponse = await openai.chat.completions.create({\n    model: config.llm.model,\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a helpful assistant. Answer based only on the provided context. Incorporate the critique to provide the most accurate response possible.'\n      },\n      {\n        role: 'user',\n        content: `Context:\\n${context}\\n\\nQuestion: ${query}\\n\\nInitial answer: ${initialResponse.choices[0].message.content}\\n\\nCritique: ${critique.choices[0].message.content}\\n\\nProvide an improved answer that addresses the critique.`\n      }\n    ]\n  });\n  \n  return refinedResponse.choices[0].message.content;\n}\n"})}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"You've now learned several advanced techniques to enhance your RAG application. In the next section, we'll explore how to deploy your application to production and optimize it for scale."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(u,{...n})}):u(n)}},8453:function(n,e,t){t.d(e,{R:function(){return o},x:function(){return i}});var r=t(6540);const s={},a=r.createContext(s);function o(n){const e=r.useContext(a);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);